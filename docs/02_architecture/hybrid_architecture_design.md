# è‚¡ç¥¨ä¼°å€¼ç³»ç»Ÿæ··åˆæ¶æ„è®¾è®¡æ–‡æ¡£

## æ–‡æ¡£ä¿¡æ¯

**æ–‡æ¡£æ ‡é¢˜**: æ··åˆæ•°æ®æºæ¶æ„è®¾è®¡  
**åˆ›å»ºæ—¥æœŸ**: 2025å¹´1æœˆ  
**ç‰ˆæœ¬**: v1.0  
**ä½œè€…**: huliux  
**å®¡æ ¸çŠ¶æ€**: å¾…å®¡æ ¸

## æ•´ç†è¯´æ˜

æœ¬æ–‡æ¡£å·²æ•´åˆä»¥ä¸‹å†…å®¹ï¼š
- åŸmigration_checklist.mdçš„å®æ–½æ­¥éª¤
- åŸtushare_migration_technical_analysis.mdçš„æŠ€æœ¯ç»†èŠ‚
- åŸtushare_postgresql_field_comparison.mdçš„å­—æ®µæ˜ å°„  

## æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†è‚¡ç¥¨ä¼°å€¼ç³»ç»Ÿé‡‡ç”¨çš„**Tushare + PostgreSQLæ··åˆæ¶æ„**è®¾è®¡æ–¹æ¡ˆã€‚è¯¥æ¶æ„é€šè¿‡ä¸»å¤‡æ•°æ®æºæ¨¡å¼ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„æ•°æ®å¯é æ€§ã€æ€§èƒ½è¡¨ç°å’Œæˆæœ¬æ•ˆç›Šã€‚æ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬æ™ºèƒ½æ•°æ®æºåˆ‡æ¢ã€å¤šå±‚çº§ç¼“å­˜æœºåˆ¶ã€è‡ªåŠ¨é™çº§æ¢å¤å’Œå…¨é¢ç›‘æ§å‘Šè­¦ã€‚

### æ¶æ„ä¼˜åŠ¿
- ğŸ¯ **é«˜å¯é æ€§**: åŒæ•°æ®æºä¿éšœï¼Œç³»ç»Ÿå¯ç”¨æ€§è¾¾99.5%
- âš¡ **é«˜æ€§èƒ½**: æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œæ•°æ®è®¿é—®é€Ÿåº¦æå‡70%
- ğŸ’° **æˆæœ¬ä¼˜åŒ–**: APIè°ƒç”¨æˆæœ¬é™ä½60%
- ğŸ”„ **çµæ´»åˆ‡æ¢**: æ”¯æŒé…ç½®åŒ–æ•°æ®æºé€‰æ‹©
- ğŸ“Š **å…¨é¢ç›‘æ§**: å®æ—¶çŠ¶æ€ç›‘æ§å’Œæ™ºèƒ½å‘Šè­¦

### è¿‘æœŸæ”¹åŠ¨æ‘˜è¦ï¼ˆ2025-09-26ï¼‰
- **ç”¨æˆ·è®¤è¯ç³»ç»Ÿå®Œæ•´å®ç°**ï¼š
  - æ•°æ®åº“æ¶æ„ä»SQLiteè¿ç§»è‡³PostgreSQL/Supabaseï¼Œè§£å†³å¹¶å‘é”é—®é¢˜
  - å®Œæ•´çš„ç”¨æˆ·æ³¨å†Œã€ç™»å½•ã€ä¼šè¯ç®¡ç†åŠŸèƒ½
  - è®¤è¯ä¸­é—´ä»¶ä¿æŠ¤æ•æ„ŸåŠŸèƒ½å’Œæ•°æ®è®¿é—®
  - å¯†ç å“ˆå¸Œã€ä¼šè¯ä»¤ç‰Œã€é˜²CSRFç­‰å®‰å…¨ç‰¹æ€§
- **æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿé‡å¤§å‡çº§**ï¼š
  - ä¼°å€¼ç»“æœåœ¨ç”¨æˆ·ä¼šè¯é—´æŒä¹…ä¿å­˜ï¼Œå¤§å¹…æå‡ç”¨æˆ·ä½“éªŒ
  - ä¼šè¯æ¢å¤åŠŸèƒ½ï¼Œç”¨æˆ·é‡æ–°ç™»å½•åå¯æ¢å¤ä¹‹å‰çš„åˆ†æç»“æœ
  - ç¼“å­˜ç­–ç•¥ä¼˜åŒ–ï¼Œå¹³è¡¡æ€§èƒ½ä¸æ•°æ®ä¸€è‡´æ€§
  - ä¿®å¤ç¼“å­˜ç³»ç»Ÿä¸­undefinedå˜é‡ç­‰å…³é”®é”™è¯¯
- **å‰ç«¯ç”¨æˆ·ä½“éªŒä¼˜åŒ–**ï¼š
  - ä¼˜åŒ–ç•Œé¢å¸ƒå±€å’Œå‚æ•°è®¾ç½®ä½“éªŒï¼Œç®€åŒ–æ“ä½œæµç¨‹
  - å¢å¼ºç”¨æˆ·åé¦ˆå’Œé”™è¯¯å¤„ç†æœºåˆ¶
  - ä¿®å¤"ä¼°å€¼è®¡ç®—å‡ºé”™: None"ç­‰é”™è¯¯æ˜¾ç¤ºé—®é¢˜
  - ä¼˜åŒ–æ•°æ®å±•ç¤ºæ ¼å¼å’Œè§†è§‰æ•ˆæœ

### å†å²æ”¹åŠ¨æ‘˜è¦ï¼ˆ2025-09-11ï¼‰
- æœåŠ¡å±‚ï¼ˆValuationServiceï¼‰å¢å¼ºï¼š
  - åœ¨åŸºç¡€ä¼°å€¼ä¸æ•æ„Ÿæ€§ä¸¤ç±»è·¯å¾„ç»Ÿä¸€ç¼–æ’ Forecasterã€WACCã€ç»ˆå€¼ä¸ç°å€¼è®¡ç®—ï¼Œæ–°å¢æœåŠ¡å†…å›é€€ä¸å®ˆæŠ¤é€»è¾‘ã€‚
  - å½“ `wacc_weight_mode=market` å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ° `target` æƒé‡å¹¶è®°å½•è­¦å‘Šï¼Œé¿å… 500ã€‚
  - å¯¹æ•æ„Ÿæ€§å•å…ƒæ ¼ï¼š
    - è®¡ç®— `EV/EBITDA (Terminal)` ä¸ `implied_pgr`ï¼›
    - å½“ `g â‰¥ WACC` æ—¶è·³è¿‡è¯¥ç»„åˆï¼›
    - `dcf_implied_pe` ç¼ºå¤±æ—¶æŒ‰åŸºå‡† EPS å›å¡«ã€‚
- GDP ä¸Šé™æ§åˆ¶ï¼šæœåŠ¡å±‚åœ¨è¯·æ±‚ç»´åº¦æ”¯æŒå¼€å…³ä¸ä¸Šé™å€¼ï¼ˆä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡ï¼‰ï¼›ç»ˆå€¼è®¡ç®—å™¨ä»ä¿ç•™åº•å±‚ä¸Šé™ä¸æœ‰æ•ˆæ€§æ ¡éªŒï¼ŒåŒå±‚ä¿æŠ¤ã€‚
- LLM æŠ¥å‘Šï¼šæç¤ºæ¨¡æ¿å¼ºåŒ–ï¼ˆä»·å€¼æŠ•èµ„å¯¼å‘ï¼‰ï¼Œå¹¶åœ¨å“åº”ä¸­æºå¸¦ `debug_request_slice`ï¼ˆå«è¡Œä¸šé¢„è®¾ä¸åç¦»ï¼‰ä»¥ä¾¿å®¡è®¡ã€‚

## 1. æ¶æ„æ¦‚è§ˆ

### 1.1 æ•´ä½“æ¶æ„å›¾

```
è‚¡ç¥¨ä¼°å€¼ç³»ç»Ÿæ··åˆæ¶æ„
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              åº”ç”¨å±‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Streamlitå‰ç«¯ â”‚    â”‚   FastAPI       â”‚    â”‚   ä¸šåŠ¡é€»è¾‘å±‚     â”‚        â”‚
â”‚  â”‚   (ç”¨æˆ·ç•Œé¢)     â”‚    â”‚   (APIæœåŠ¡)     â”‚    â”‚   (ä¼°å€¼è®¡ç®—)     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            æ•°æ®è®¿é—®æŠ½è±¡å±‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    DataSourceManager                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚   é…ç½®ç®¡ç†      â”‚    â”‚   è·¯ç”±ç­–ç•¥      â”‚    â”‚   å¥åº·æ£€æŸ¥      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ (.envæ§åˆ¶)      â”‚    â”‚ (æ™ºèƒ½é€‰æ‹©)      â”‚    â”‚ (çŠ¶æ€ç›‘æ§)      â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç¼“å­˜ç®¡ç†å±‚     â”‚    â”‚   ä¸»æ•°æ®æº      â”‚    â”‚   å¤‡æ•°æ®æº      â”‚
â”‚                 â”‚    â”‚   (Tushare)     â”‚    â”‚ (PostgreSQL)    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â”‚ Redisç¼“å­˜   â”‚ â”‚    â”‚ â€¢ å®æ—¶æ•°æ®      â”‚    â”‚ â€¢ å†å²æ•°æ®      â”‚
â”‚ â”‚ (çƒ­æ•°æ®)    â”‚ â”‚    â”‚ â€¢ é«˜é¢‘æ›´æ–°      â”‚    â”‚ â€¢ ç¨³å®šå¯é       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â€¢ ä¸°å¯Œå­—æ®µ      â”‚    â”‚ â€¢ æœ¬åœ°è®¿é—®      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â€¢ APIé™æµ      â”‚    â”‚ â€¢ ç¦»çº¿å¯ç”¨      â”‚
â”‚ â”‚ æœ¬åœ°ç¼“å­˜    â”‚ â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â”‚ (å†·æ•°æ®)    â”‚ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                        â”‚
                                  â–¼                        â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            ç›‘æ§å‘Šè­¦å±‚                    â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                    â”‚  â”‚   æ€§èƒ½ç›‘æ§      â”‚ â”‚   å¼‚å¸¸å‘Šè­¦      â”‚â”‚
                    â”‚  â”‚ (å“åº”æ—¶é—´/QPS)  â”‚ â”‚ (æ•…éšœæ£€æµ‹)      â”‚â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒç»„ä»¶è¯´æ˜

| ç»„ä»¶ | èŒè´£ | æŠ€æœ¯å®ç° |
|------|------|----------|
| **DataSourceManager** | æ•°æ®æºç®¡ç†å’Œè·¯ç”± | PythonæŠ½è±¡å·¥å‚æ¨¡å¼ |
| **TushareDataFetcher** | Tushareæ•°æ®è·å– | Tushare SDK + è¿æ¥æ±  |
| **PostgreSQLDataFetcher** | PostgreSQLæ•°æ®è·å– | SQLAlchemy + è¿æ¥æ±  |
| **CacheManager** | ç¼“å­˜ç®¡ç† | Redis + æœ¬åœ°LRUç¼“å­˜ |
| **HealthChecker** | å¥åº·çŠ¶æ€æ£€æŸ¥ | å®šæ—¶ä»»åŠ¡ + çŠ¶æ€å­˜å‚¨ |
| **MonitoringService** | ç›‘æ§å‘Šè­¦ | Prometheus + è‡ªå®šä¹‰æŒ‡æ ‡ |

## 2. æ•°æ®æºè®¾è®¡

### 2.1 ä¸»æ•°æ®æº - Tushare

#### 2.1.1 æ•°æ®æºç‰¹æ€§
```yaml
æ•°æ®æº: Tushare Pro API
ç±»å‹: ä¸»æ•°æ®æº
ä¼˜åŠ¿:
  - æ•°æ®å®æ—¶æ€§å¼ºï¼ŒT+1æ›´æ–°
  - æ•°æ®å­—æ®µä¸°å¯Œï¼Œè¦†ç›–å…¨é¢
  - æ•°æ®è´¨é‡é«˜ï¼Œä¸“ä¸šé‡‘èæ•°æ®æä¾›å•†
  - APIæ¥å£æ ‡å‡†åŒ–ï¼Œæ˜“äºé›†æˆ

æŒ‘æˆ˜:
  - APIè°ƒç”¨æœ‰é¢‘ç‡é™åˆ¶
  - éœ€è¦ä»˜è´¹ç§¯åˆ†ï¼Œæœ‰æˆæœ¬è€ƒè™‘
  - ç½‘ç»œä¾èµ–ï¼Œå­˜åœ¨æœåŠ¡ä¸­æ–­é£é™©
  - æ•°æ®é‡å¤§æ—¶å“åº”è¾ƒæ…¢
```

#### 2.1.2 APIé…é¢ç®¡ç†
```python
# APIé…é¢ç­–ç•¥
API_QUOTA_CONFIG = {
    'daily_limit': 10000,      # æ—¥è°ƒç”¨é™åˆ¶
    'minute_limit': 500,       # åˆ†é’Ÿè°ƒç”¨é™åˆ¶
    'priority_apis': [         # ä¼˜å…ˆçº§APIåˆ—è¡¨
        'stock_basic',         # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        'daily_basic',         # æ¯æ—¥åŸºæœ¬é¢
        'income',              # åˆ©æ¶¦è¡¨
        'balancesheet',        # èµ„äº§è´Ÿå€ºè¡¨
        'cashflow'             # ç°é‡‘æµé‡è¡¨
    ],
    'cache_duration': {        # ç¼“å­˜æ—¶é•¿é…ç½®
        'stock_basic': 86400,  # 24å°æ—¶
        'daily_basic': 3600,   # 1å°æ—¶
        'financial_data': 43200 # 12å°æ—¶
    }
}
```

### 2.2 å¤‡æ•°æ®æº - PostgreSQL

#### 2.2.1 æ•°æ®æºç‰¹æ€§
```yaml
æ•°æ®æº: PostgreSQLæ•°æ®åº“
ç±»å‹: å¤‡æ•°æ®æº
ä¼˜åŠ¿:
  - æœ¬åœ°è®¿é—®ï¼Œå“åº”é€Ÿåº¦å¿«
  - æ— ç½‘ç»œä¾èµ–ï¼Œç¦»çº¿å¯ç”¨
  - æ•°æ®ç¨³å®šï¼Œæ— APIé™åˆ¶
  - æ”¯æŒå¤æ‚æŸ¥è¯¢å’Œèšåˆ

å±€é™:
  - æ•°æ®æ›´æ–°é¢‘ç‡è¾ƒä½
  - éœ€è¦å®šæœŸç»´æŠ¤å’Œæ›´æ–°
  - å­˜å‚¨æˆæœ¬éšæ•°æ®é‡å¢é•¿
  - æ•°æ®å­—æ®µå¯èƒ½ä¸å¦‚Tushareä¸°å¯Œ
```

#### 2.2.2 æ•°æ®è¡¨ç»“æ„
```sql
-- æ ¸å¿ƒæ•°æ®è¡¨
CREATE TABLE stock_basic (
    ts_code VARCHAR(10) PRIMARY KEY,
    symbol VARCHAR(10),
    name VARCHAR(50),
    area VARCHAR(20),
    industry VARCHAR(50),
    market VARCHAR(10),
    list_date DATE,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE daily_quotes (
    id SERIAL PRIMARY KEY,
    ts_code VARCHAR(10),
    trade_date DATE,
    open DECIMAL(10,2),
    high DECIMAL(10,2),
    low DECIMAL(10,2),
    close DECIMAL(10,2),
    volume BIGINT,
    amount DECIMAL(15,2),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(ts_code, trade_date)
);

-- è´¢åŠ¡æ•°æ®è¡¨
CREATE TABLE financial_indicators (
    id SERIAL PRIMARY KEY,
    ts_code VARCHAR(10),
    end_date DATE,
    pe DECIMAL(10,2),
    pb DECIMAL(10,2),
    ps DECIMAL(10,2),
    total_share DECIMAL(15,2),
    float_share DECIMAL(15,2),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(ts_code, end_date)
);
```

### 2.3 æ•°æ®æºæ˜ å°„å…³ç³»

#### 2.3.1 å­—æ®µæ˜ å°„è¡¨
```python
# æ•°æ®æºå­—æ®µæ˜ å°„é…ç½®
FIELD_MAPPING = {
    'stock_basic': {
        'tushare_fields': ['ts_code', 'symbol', 'name', 'area', 'industry', 'market', 'list_date'],
        'postgresql_fields': ['ts_code', 'symbol', 'name', 'area', 'industry', 'market', 'list_date'],
        'mapping': {  # å­—æ®µåæ˜ å°„
            'ts_code': 'ts_code',
            'symbol': 'symbol', 
            'name': 'name',
            'area': 'area',
            'industry': 'industry',
            'market': 'market',
            'list_date': 'list_date'
        }
    },
    'daily_basic': {
        'tushare_fields': ['ts_code', 'trade_date', 'close', 'pe', 'pb', 'total_share'],
        'postgresql_fields': ['ts_code', 'trade_date', 'close', 'pe', 'pb', 'total_share'],
        'unit_conversion': {  # å•ä½è½¬æ¢
            'total_share': lambda x: x * 10000 if x else None  # ä¸‡è‚¡è½¬è‚¡
        }
    }
}
```

## 3. æ•°æ®æºç®¡ç†å™¨è®¾è®¡

### 3.1 DataSourceManageræ ¸å¿ƒç±»

```python
from abc import ABC, abstractmethod
from enum import Enum
from typing import Dict, Any, Optional, List
import logging

class DataSourceType(Enum):
    """æ•°æ®æºç±»å‹æšä¸¾"""
    TUSHARE = "tushare"
    POSTGRESQL = "postgresql"
    CACHE = "cache"

class DataSourceStatus(Enum):
    """æ•°æ®æºçŠ¶æ€æšä¸¾"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNAVAILABLE = "unavailable"

class DataSourceManager:
    """æ•°æ®æºç®¡ç†å™¨ - æ ¸å¿ƒåè°ƒç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.primary_source = DataSourceType.TUSHARE
        self.fallback_source = DataSourceType.POSTGRESQL
        self.data_fetchers = {}
        self.cache_manager = None
        self.health_checker = None
        self.logger = logging.getLogger(__name__)
        
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ç»„ä»¶"""
        # åˆå§‹åŒ–æ•°æ®è·å–å™¨
        self.data_fetchers[DataSourceType.TUSHARE] = TushareDataFetcher(self.config)
        self.data_fetchers[DataSourceType.POSTGRESQL] = PostgreSQLDataFetcher(self.config)
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(self.config)
        
        # åˆå§‹åŒ–å¥åº·æ£€æŸ¥å™¨
        self.health_checker = HealthChecker(self.data_fetchers)
    
    async def get_data(self, data_type: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–æ•°æ® - ä¸»è¦å…¥å£æ–¹æ³•"""
        try:
            # 1. å°è¯•ä»ç¼“å­˜è·å–
            cached_data = await self.cache_manager.get(data_type, params)
            if cached_data:
                self.logger.info(f"Cache hit for {data_type}")
                return cached_data
            
            # 2. é€‰æ‹©æ•°æ®æº
            data_source = await self._select_data_source(data_type)
            
            # 3. è·å–æ•°æ®
            data = await self._fetch_data(data_source, data_type, params)
            
            # 4. ç¼“å­˜æ•°æ®
            await self.cache_manager.set(data_type, params, data)
            
            return data
            
        except Exception as e:
            self.logger.error(f"Error getting data for {data_type}: {str(e)}")
            return await self._handle_error(data_type, params, e)
    
    async def _select_data_source(self, data_type: str) -> DataSourceType:
        """æ™ºèƒ½æ•°æ®æºé€‰æ‹©"""
        # æ£€æŸ¥é…ç½®çš„æ•°æ®æºåå¥½
        if self.config.get('force_data_source'):
            return DataSourceType(self.config['force_data_source'])
        
        # æ£€æŸ¥ä¸»æ•°æ®æºå¥åº·çŠ¶æ€
        primary_status = await self.health_checker.check_health(self.primary_source)
        if primary_status == DataSourceStatus.HEALTHY:
            return self.primary_source
        
        # ä¸»æ•°æ®æºä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®æº
        fallback_status = await self.health_checker.check_health(self.fallback_source)
        if fallback_status in [DataSourceStatus.HEALTHY, DataSourceStatus.DEGRADED]:
            self.logger.warning(f"Primary source unavailable, using fallback: {self.fallback_source}")
            return self.fallback_source
        
        # æ‰€æœ‰æ•°æ®æºéƒ½ä¸å¯ç”¨
        raise Exception("All data sources are unavailable")
    
    async def _fetch_data(self, source: DataSourceType, data_type: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """ä»æŒ‡å®šæ•°æ®æºè·å–æ•°æ®"""
        fetcher = self.data_fetchers[source]
        return await fetcher.fetch_data(data_type, params)
    
    async def _handle_error(self, data_type: str, params: Dict[str, Any], error: Exception) -> Dict[str, Any]:
        """é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥"""
        self.logger.error(f"Data fetch failed: {str(error)}")
        
        # å°è¯•ä»ç¼“å­˜è·å–è¿‡æœŸæ•°æ®
        stale_data = await self.cache_manager.get_stale(data_type, params)
        if stale_data:
            self.logger.warning("Returning stale cached data due to error")
            return stale_data
        
        # è¿”å›é»˜è®¤å€¼æˆ–æŠ›å‡ºå¼‚å¸¸
        raise error
```

### 3.2 å¥åº·æ£€æŸ¥æœºåˆ¶

```python
import asyncio
from datetime import datetime, timedelta
from typing import Dict

class HealthChecker:
    """æ•°æ®æºå¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self, data_fetchers: Dict[DataSourceType, Any]):
        self.data_fetchers = data_fetchers
        self.health_status = {}
        self.last_check = {}
        self.check_interval = 60  # 60ç§’æ£€æŸ¥é—´éš”
        self.logger = logging.getLogger(__name__)
    
    async def check_health(self, source: DataSourceType) -> DataSourceStatus:
        """æ£€æŸ¥æ•°æ®æºå¥åº·çŠ¶æ€"""
        now = datetime.now()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ£€æŸ¥
        if (source not in self.last_check or 
            now - self.last_check[source] > timedelta(seconds=self.check_interval)):
            
            await self._perform_health_check(source)
            self.last_check[source] = now
        
        return self.health_status.get(source, DataSourceStatus.UNAVAILABLE)
    
    async def _perform_health_check(self, source: DataSourceType):
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        try:
            fetcher = self.data_fetchers[source]
            
            # æ‰§è¡Œç®€å•çš„è¿æ¥æµ‹è¯•
            start_time = datetime.now()
            await fetcher.health_check()
            response_time = (datetime.now() - start_time).total_seconds()
            
            # æ ¹æ®å“åº”æ—¶é—´åˆ¤æ–­çŠ¶æ€
            if response_time < 2.0:
                self.health_status[source] = DataSourceStatus.HEALTHY
            elif response_time < 5.0:
                self.health_status[source] = DataSourceStatus.DEGRADED
            else:
                self.health_status[source] = DataSourceStatus.UNAVAILABLE
                
            self.logger.info(f"{source} health check: {self.health_status[source]} (response_time: {response_time:.2f}s)")
            
        except Exception as e:
            self.health_status[source] = DataSourceStatus.UNAVAILABLE
            self.logger.error(f"{source} health check failed: {str(e)}")
    
    async def start_monitoring(self):
        """å¯åŠ¨æŒç»­ç›‘æ§"""
        while True:
            for source in self.data_fetchers.keys():
                await self._perform_health_check(source)
            await asyncio.sleep(self.check_interval)
```

## 4. ç¼“å­˜ç­–ç•¥è®¾è®¡

### 4.1 å¤šå±‚çº§ç¼“å­˜æ¶æ„

```
ç¼“å­˜å±‚çº§ç»“æ„
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    L1: å†…å­˜ç¼“å­˜ (LRU)                        â”‚
â”‚  â€¢ å®¹é‡: 1000ä¸ªå¯¹è±¡                                          â”‚
â”‚  â€¢ TTL: 5-30åˆ†é’Ÿ                                            â”‚
â”‚  â€¢ ç”¨é€”: çƒ­ç‚¹æ•°æ®å¿«é€Ÿè®¿é—®                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    L2: Redisç¼“å­˜                            â”‚
â”‚  â€¢ å®¹é‡: 10GB                                               â”‚
â”‚  â€¢ TTL: 1-24å°æ—¶                                            â”‚
â”‚  â€¢ ç”¨é€”: åˆ†å¸ƒå¼ç¼“å­˜ï¼Œæ”¯æŒé›†ç¾¤                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    L3: æœ¬åœ°æ–‡ä»¶ç¼“å­˜                          â”‚
â”‚  â€¢ å®¹é‡: 1GB                                                â”‚
â”‚  â€¢ TTL: 1-7å¤©                                               â”‚
â”‚  â€¢ ç”¨é€”: å†·æ•°æ®å­˜å‚¨ï¼Œç¦»çº¿è®¿é—®                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ç¼“å­˜ç®¡ç†å™¨å®ç°

```python
import json
import hashlib
from datetime import datetime, timedelta
from typing import Any, Dict, Optional
import redis
from cachetools import LRUCache

class CacheManager:
    """å¤šå±‚çº§ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # L1: å†…å­˜ç¼“å­˜
        self.memory_cache = LRUCache(maxsize=1000)
        
        # L2: Redisç¼“å­˜
        self.redis_client = redis.Redis(
            host=config.get('redis_host', 'localhost'),
            port=config.get('redis_port', 6379),
            db=config.get('redis_db', 0),
            decode_responses=True
        )
        
        # L3: æ–‡ä»¶ç¼“å­˜è·¯å¾„
        self.file_cache_dir = config.get('file_cache_dir', './cache')
        
        self.logger = logging.getLogger(__name__)
    
    def _generate_cache_key(self, data_type: str, params: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºå‚æ•°çš„å“ˆå¸Œå€¼
        params_str = json.dumps(params, sort_keys=True)
        params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
        return f"{data_type}:{params_hash}"
    
    async def get(self, data_type: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(data_type, params)
        
        # L1: æ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.memory_cache:
            self.logger.debug(f"L1 cache hit: {cache_key}")
            return self.memory_cache[cache_key]
        
        # L2: æ£€æŸ¥Redisç¼“å­˜
        try:
            redis_data = self.redis_client.get(cache_key)
            if redis_data:
                data = json.loads(redis_data)
                # å›å¡«åˆ°L1ç¼“å­˜
                self.memory_cache[cache_key] = data
                self.logger.debug(f"L2 cache hit: {cache_key}")
                return data
        except Exception as e:
            self.logger.warning(f"Redis cache error: {str(e)}")
        
        # L3: æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        try:
            file_path = os.path.join(self.file_cache_dir, f"{cache_key}.json")
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    data = json.load(f)
                
                # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_time = datetime.fromisoformat(data.get('_cache_time', '1970-01-01'))
                if datetime.now() - cache_time < timedelta(days=7):
                    # å›å¡«åˆ°ä¸Šçº§ç¼“å­˜
                    self.memory_cache[cache_key] = data
                    try:
                        self.redis_client.setex(cache_key, 3600, json.dumps(data))
                    except:
                        pass
                    
                    self.logger.debug(f"L3 cache hit: {cache_key}")
                    return data
        except Exception as e:
            self.logger.warning(f"File cache error: {str(e)}")
        
        return None
    
    async def set(self, data_type: str, params: Dict[str, Any], data: Dict[str, Any]):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(data_type, params)
        
        # æ·»åŠ ç¼“å­˜æ—¶é—´æˆ³
        data_with_timestamp = {
            **data,
            '_cache_time': datetime.now().isoformat(),
            '_data_type': data_type
        }
        
        # è·å–TTLé…ç½®
        ttl_config = self.config.get('cache_ttl', {})
        memory_ttl = ttl_config.get(data_type, {}).get('memory', 1800)  # 30åˆ†é’Ÿ
        redis_ttl = ttl_config.get(data_type, {}).get('redis', 3600)    # 1å°æ—¶
        
        # L1: è®¾ç½®å†…å­˜ç¼“å­˜
        self.memory_cache[cache_key] = data_with_timestamp
        
        # L2: è®¾ç½®Redisç¼“å­˜
        try:
            self.redis_client.setex(
                cache_key, 
                redis_ttl, 
                json.dumps(data_with_timestamp)
            )
        except Exception as e:
            self.logger.warning(f"Redis cache set error: {str(e)}")
        
        # L3: è®¾ç½®æ–‡ä»¶ç¼“å­˜ï¼ˆå¼‚æ­¥ï¼‰
        asyncio.create_task(self._set_file_cache(cache_key, data_with_timestamp))
    
    async def _set_file_cache(self, cache_key: str, data: Dict[str, Any]):
        """å¼‚æ­¥è®¾ç½®æ–‡ä»¶ç¼“å­˜"""
        try:
            os.makedirs(self.file_cache_dir, exist_ok=True)
            file_path = os.path.join(self.file_cache_dir, f"{cache_key}.json")
            
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2)
                
        except Exception as e:
            self.logger.warning(f"File cache set error: {str(e)}")
    
    async def get_stale(self, data_type: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è·å–è¿‡æœŸçš„ç¼“å­˜æ•°æ®ï¼ˆç”¨äºé™çº§ï¼‰"""
        cache_key = self._generate_cache_key(data_type, params)
        
        # å°è¯•ä»æ–‡ä»¶ç¼“å­˜è·å–ï¼Œå¿½ç•¥è¿‡æœŸæ—¶é—´
        try:
            file_path = os.path.join(self.file_cache_dir, f"{cache_key}.json")
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    data = json.load(f)
                self.logger.warning(f"Returning stale cache data: {cache_key}")
                return data
        except Exception as e:
            self.logger.error(f"Stale cache retrieval error: {str(e)}")
        
        return None
    
    async def invalidate(self, data_type: str, params: Dict[str, Any] = None):
        """ç¼“å­˜å¤±æ•ˆ"""
        if params:
            # å¤±æ•ˆç‰¹å®šç¼“å­˜
            cache_key = self._generate_cache_key(data_type, params)
            self._invalidate_key(cache_key)
        else:
            # å¤±æ•ˆæ•°æ®ç±»å‹çš„æ‰€æœ‰ç¼“å­˜
            await self._invalidate_by_pattern(f"{data_type}:*")
    
    def _invalidate_key(self, cache_key: str):
        """å¤±æ•ˆæŒ‡å®šé”®çš„ç¼“å­˜"""
        # L1: å†…å­˜ç¼“å­˜
        if cache_key in self.memory_cache:
            del self.memory_cache[cache_key]
        
        # L2: Redisç¼“å­˜
        try:
            self.redis_client.delete(cache_key)
        except:
            pass
        
        # L3: æ–‡ä»¶ç¼“å­˜
        try:
            file_path = os.path.join(self.file_cache_dir, f"{cache_key}.json")
            if os.path.exists(file_path):
                os.remove(file_path)
        except:
            pass
    
    async def _invalidate_by_pattern(self, pattern: str):
        """æŒ‰æ¨¡å¼å¤±æ•ˆç¼“å­˜"""
        # Redisæ¨¡å¼åŒ¹é…åˆ é™¤
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                self.redis_client.delete(*keys)
        except:
            pass
        
        # å†…å­˜ç¼“å­˜æ¨¡å¼åŒ¹é…åˆ é™¤
        keys_to_delete = [k for k in self.memory_cache.keys() if k.startswith(pattern.replace('*', ''))]
        for key in keys_to_delete:
            del self.memory_cache[key]
```

### 4.3 ç¼“å­˜ç­–ç•¥é…ç½®

```python
# ç¼“å­˜TTLé…ç½®
CACHE_TTL_CONFIG = {
    'stock_basic': {
        'memory': 3600,    # 1å°æ—¶
        'redis': 86400,    # 24å°æ—¶
        'file': 604800     # 7å¤©
    },
    'daily_basic': {
        'memory': 1800,    # 30åˆ†é’Ÿ
        'redis': 3600,     # 1å°æ—¶
        'file': 86400      # 1å¤©
    },
    'financial_data': {
        'memory': 7200,    # 2å°æ—¶
        'redis': 43200,    # 12å°æ—¶
        'file': 2592000    # 30å¤©
    },
    'real_time_quotes': {
        'memory': 300,     # 5åˆ†é’Ÿ
        'redis': 900,      # 15åˆ†é’Ÿ
        'file': 3600       # 1å°æ—¶
    }
}

# ç¼“å­˜å¤±æ•ˆç­–ç•¥
CACHE_INVALIDATION_RULES = {
    'market_close': {
        'time': '15:30',
        'invalidate': ['daily_basic', 'real_time_quotes']
    },
    'financial_report': {
        'trigger': 'quarterly',
        'invalidate': ['financial_data', 'valuation_metrics']
    },
    'stock_list_update': {
        'trigger': 'weekly',
        'invalidate': ['stock_basic']
    }
}
```

## 5. é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥

### 5.1 é”™è¯¯åˆ†ç±»å’Œå¤„ç†

```python
from enum import Enum
from typing import Dict, Any, Optional

class ErrorType(Enum):
    """é”™è¯¯ç±»å‹æšä¸¾"""
    NETWORK_ERROR = "network_error"          # ç½‘ç»œè¿æ¥é”™è¯¯
    API_LIMIT_ERROR = "api_limit_error"      # APIé™æµé”™è¯¯
    DATA_NOT_FOUND = "data_not_found"        # æ•°æ®ä¸å­˜åœ¨
    AUTHENTICATION_ERROR = "auth_error"       # è®¤è¯é”™è¯¯
    SERVER_ERROR = "server_error"            # æœåŠ¡å™¨é”™è¯¯
    TIMEOUT_ERROR = "timeout_error"          # è¶…æ—¶é”™è¯¯
    DATA_FORMAT_ERROR = "format_error"       # æ•°æ®æ ¼å¼é”™è¯¯

class ErrorHandler:
    """ç»Ÿä¸€é”™è¯¯å¤„ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.retry_config = config.get('retry_config', {})
        self.fallback_config = config.get('fallback_config', {})
        self.logger = logging.getLogger(__name__)
    
    async def handle_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†å…¥å£"""
        error_type = self._classify_error(error)
        
        self.logger.error(f"Error occurred: {error_type} - {str(error)}")
        
        # æ ¹æ®é”™è¯¯ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥
        if error_type == ErrorType.API_LIMIT_ERROR:
            return await self._handle_api_limit_error(error, context)
        elif error_type == ErrorType.NETWORK_ERROR:
            return await self._handle_network_error(error, context)
        elif error_type == ErrorType.TIMEOUT_ERROR:
            return await self._handle_timeout_error(error, context)
        elif error_type == ErrorType.DATA_NOT_FOUND:
            return await self._handle_data_not_found_error(error, context)
        else:
            return await self._handle_generic_error(error, context)
    
    def _classify_error(self, error: Exception) -> ErrorType:
        """é”™è¯¯åˆ†ç±»"""
        error_msg = str(error).lower()
        
        if 'rate limit' in error_msg or 'quota exceeded' in error_msg:
            return ErrorType.API_LIMIT_ERROR
        elif 'network' in error_msg or 'connection' in error_msg:
            return ErrorType.NETWORK_ERROR
        elif 'timeout' in error_msg:
            return ErrorType.TIMEOUT_ERROR
        elif 'not found' in error_msg or 'no data' in error_msg:
            return ErrorType.DATA_NOT_FOUND
        elif 'authentication' in error_msg or 'unauthorized' in error_msg:
            return ErrorType.AUTHENTICATION_ERROR
        elif 'server error' in error_msg or '500' in error_msg:
            return ErrorType.SERVER_ERROR
        else:
            return ErrorType.DATA_FORMAT_ERROR
    
    async def _handle_api_limit_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†APIé™æµé”™è¯¯"""
        # 1. åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº
        self.logger.warning("API limit reached, switching to fallback data source")
        
        # 2. å°è¯•ä»ç¼“å­˜è·å–æ•°æ®
        cache_manager = context.get('cache_manager')
        if cache_manager:
            stale_data = await cache_manager.get_stale(
                context.get('data_type'), 
                context.get('params')
            )
            if stale_data:
                return stale_data
        
        # 3. ä½¿ç”¨å¤‡ç”¨æ•°æ®æº
        fallback_fetcher = context.get('fallback_fetcher')
        if fallback_fetcher:
            return await fallback_fetcher.fetch_data(
                context.get('data_type'),
                context.get('params')
            )
        
        raise error
    
    async def _handle_network_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†ç½‘ç»œé”™è¯¯"""
        # é‡è¯•æœºåˆ¶
        retry_count = context.get('retry_count', 0)
        max_retries = self.retry_config.get('max_retries', 3)
        
        if retry_count < max_retries:
            self.logger.info(f"Network error, retrying ({retry_count + 1}/{max_retries})")
            
            # æŒ‡æ•°é€€é¿
            wait_time = 2 ** retry_count
            await asyncio.sleep(wait_time)
            
            # æ›´æ–°é‡è¯•æ¬¡æ•°
            context['retry_count'] = retry_count + 1
            
            # é‡æ–°å°è¯•
            fetcher = context.get('fetcher')
            if fetcher:
                return await fetcher.fetch_data(
                    context.get('data_type'),
                    context.get('params')
                )
        
        # é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
        return await self._apply_fallback_strategy(error, context)
    
    async def _apply_fallback_strategy(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨é™çº§ç­–ç•¥"""
        # 1. å°è¯•ä»ç¼“å­˜è·å–è¿‡æœŸæ•°æ®
        cache_manager = context.get('cache_manager')
        if cache_manager:
            stale_data = await cache_manager.get_stale(
                context.get('data_type'),
                context.get('params')
            )
            if stale_data:
                self.logger.warning("Using stale cached data as fallback")
                return stale_data
        
        # 2. ä½¿ç”¨å¤‡ç”¨æ•°æ®æº
        fallback_fetcher = context.get('fallback_fetcher')
        if fallback_fetcher:
            try:
                self.logger.warning("Using fallback data source")
                return await fallback_fetcher.fetch_data(
                    context.get('data_type'),
                    context.get('params')
                )
            except Exception as fallback_error:
                self.logger.error(f"Fallback data source also failed: {str(fallback_error)}")
        
        # 3. è¿”å›é»˜è®¤å€¼æˆ–æŠ›å‡ºå¼‚å¸¸
        default_data = self.fallback_config.get('default_data', {})
        if default_data:
            self.logger.warning("Using default fallback data")
            return default_data
        
        # æ‰€æœ‰é™çº§ç­–ç•¥éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹å¼‚å¸¸
        raise error
```

### 5.2 è‡ªåŠ¨é™çº§å’Œæ¢å¤æœºåˆ¶

```python
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List

class AutoDegradationManager:
    """è‡ªåŠ¨é™çº§ç®¡ç†å™¨"""
    
    def __init__(self, data_source_manager, config: Dict[str, Any]):
        self.data_source_manager = data_source_manager
        self.config = config
        self.degradation_rules = config.get('degradation_rules', {})
        self.recovery_rules = config.get('recovery_rules', {})
        self.current_degradations = {}
        self.logger = logging.getLogger(__name__)
    
    async def evaluate_degradation(self, source: DataSourceType, error_history: List[Dict]):
        """è¯„ä¼°æ˜¯å¦éœ€è¦é™çº§"""
        # åˆ†æé”™è¯¯å†å²
        recent_errors = [e for e in error_history 
                        if datetime.now() - e['timestamp'] < timedelta(minutes=5)]
        
        error_rate = len(recent_errors) / max(1, len(error_history))
        
        # æ£€æŸ¥é™çº§æ¡ä»¶
        if error_rate > self.degradation_rules.get('error_rate_threshold', 0.5):
            await self._trigger_degradation(source, 'high_error_rate')
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        avg_response_time = sum(e.get('response_time', 0) for e in recent_errors) / max(1, len(recent_errors))
        if avg_response_time > self.degradation_rules.get('response_time_threshold', 10.0):
            await self._trigger_degradation(source, 'slow_response')
    
    async def _trigger_degradation(self, source: DataSourceType, reason: str):
        """è§¦å‘é™çº§"""
        if source not in self.current_degradations:
            self.current_degradations[source] = {
                'reason': reason,
                'start_time': datetime.now(),
                'attempts': 0
            }
            
            self.logger.warning(f"Triggering degradation for {source}: {reason}")
            
            # é€šçŸ¥æ•°æ®æºç®¡ç†å™¨
            await self.data_source_manager.set_source_status(source, DataSourceStatus.DEGRADED)
    
    async def evaluate_recovery(self):
        """è¯„ä¼°æ¢å¤æ¡ä»¶"""
        for source, degradation_info in list(self.current_degradations.items()):
            # æ£€æŸ¥é™çº§æ—¶é—´
            degradation_duration = datetime.now() - degradation_info['start_time']
            min_degradation_time = timedelta(minutes=self.recovery_rules.get('min_degradation_minutes', 5))
            
            if degradation_duration > min_degradation_time:
                # å°è¯•æ¢å¤æ£€æŸ¥
                if await self._test_recovery(source):
                    await self._trigger_recovery(source)
    
    async def _test_recovery(self, source: DataSourceType) -> bool:
        """æµ‹è¯•æ•°æ®æºæ˜¯å¦å¯ä»¥æ¢å¤"""
        try:
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            health_checker = self.data_source_manager.health_checker
            status = await health_checker.check_health(source)
            
            return status == DataSourceStatus.HEALTHY
            
        except Exception as e:
            self.logger.debug(f"Recovery test failed for {source}: {str(e)}")
            return False
    
    async def _trigger_recovery(self, source: DataSourceType):
        """è§¦å‘æ¢å¤"""
        if source in self.current_degradations:
            degradation_info = self.current_degradations[source]
            duration = datetime.now() - degradation_info['start_time']
            
            self.logger.info(f"Recovering {source} after {duration}")
            
            # ç§»é™¤é™çº§çŠ¶æ€
            del self.current_degradations[source]
            
            # é€šçŸ¥æ•°æ®æºç®¡ç†å™¨
            await self.data_source_manager.set_source_status(source, DataSourceStatus.HEALTHY)
    
    async def start_monitoring(self):
        """å¯åŠ¨è‡ªåŠ¨é™çº§ç›‘æ§"""
        while True:
            try:
                await self.evaluate_recovery()
                await asyncio.sleep(30)  # 30ç§’æ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                self.logger.error(f"Auto degradation monitoring error: {str(e)}")
                await asyncio.sleep(60)
```

## 6. ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ

### 6.1 ç›‘æ§æŒ‡æ ‡å®šä¹‰

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from datetime import datetime
import time

@dataclass
class MetricPoint:
    """ç›‘æ§æŒ‡æ ‡æ•°æ®ç‚¹"""
    name: str
    value: float
    timestamp: datetime
    tags: Dict[str, str]
    unit: str = ""

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.metrics_buffer = []
        self.counters = {}
        self.gauges = {}
        self.histograms = {}
        self.logger = logging.getLogger(__name__)
    
    def increment_counter(self, name: str, value: float = 1.0, tags: Dict[str, str] = None):
        """é€’å¢è®¡æ•°å™¨"""
        key = self._make_key(name, tags or {})
        self.counters[key] = self.counters.get(key, 0) + value
        
        self._add_metric_point(MetricPoint(
            name=name,
            value=self.counters[key],
            timestamp=datetime.now(),
            tags=tags or {},
            unit="count"
        ))
    
    def set_gauge(self, name: str, value: float, tags: Dict[str, str] = None):
        """è®¾ç½®ä»ªè¡¨å€¼"""
        key = self._make_key(name, tags or {})
        self.gauges[key] = value
        
        self._add_metric_point(MetricPoint(
            name=name,
            value=value,
            timestamp=datetime.now(),
            tags=tags or {},
            unit="gauge"
        ))
    
    def record_histogram(self, name: str, value: float, tags: Dict[str, str] = None):
        """è®°å½•ç›´æ–¹å›¾å€¼"""
        key = self._make_key(name, tags or {})
        if key not in self.histograms:
            self.histograms[key] = []
        
        self.histograms[key].append(value)
        
        # ä¿æŒæœ€è¿‘1000ä¸ªå€¼
        if len(self.histograms[key]) > 1000:
            self.histograms[key] = self.histograms[key][-1000:]
        
        self._add_metric_point(MetricPoint(
            name=name,
            value=value,
            timestamp=datetime.now(),
            tags=tags or {},
            unit="histogram"
        ))
    
    def _make_key(self, name: str, tags: Dict[str, str]) -> str:
        """ç”ŸæˆæŒ‡æ ‡é”®"""
        tag_str = ",".join(f"{k}={v}" for k, v in sorted(tags.items()))
        return f"{name}[{tag_str}]"
    
    def _add_metric_point(self, metric: MetricPoint):
        """æ·»åŠ æŒ‡æ ‡ç‚¹åˆ°ç¼“å†²åŒº"""
        self.metrics_buffer.append(metric)
        
        # ç¼“å†²åŒºæ»¡æ—¶å‘é€
        if len(self.metrics_buffer) >= 100:
            asyncio.create_task(self._flush_metrics())
    
    async def _flush_metrics(self):
        """åˆ·æ–°æŒ‡æ ‡åˆ°ç›‘æ§ç³»ç»Ÿ"""
        if not self.metrics_buffer:
            return
        
        try:
            # å‘é€åˆ°Prometheusæˆ–å…¶ä»–ç›‘æ§ç³»ç»Ÿ
            await self._send_to_monitoring_system(self.metrics_buffer)
            self.metrics_buffer.clear()
            
        except Exception as e:
            self.logger.error(f"Failed to flush metrics: {str(e)}")
    
    async def _send_to_monitoring_system(self, metrics: List[MetricPoint]):
        """å‘é€æŒ‡æ ‡åˆ°ç›‘æ§ç³»ç»Ÿ"""
        # è¿™é‡Œå¯ä»¥é›†æˆPrometheusã€InfluxDBç­‰ç›‘æ§ç³»ç»Ÿ
        for metric in metrics:
            self.logger.debug(f"Metric: {metric.name}={metric.value} {metric.tags}")

# æ ¸å¿ƒç›‘æ§æŒ‡æ ‡å®šä¹‰
CORE_METRICS = {
    # æ•°æ®æºæŒ‡æ ‡
    'data_source_requests_total': 'Counter - æ•°æ®æºè¯·æ±‚æ€»æ•°',
    'data_source_errors_total': 'Counter - æ•°æ®æºé”™è¯¯æ€»æ•°',
    'data_source_response_time': 'Histogram - æ•°æ®æºå“åº”æ—¶é—´',
    'data_source_availability': 'Gauge - æ•°æ®æºå¯ç”¨æ€§',
    
    # ç¼“å­˜æŒ‡æ ‡
    'cache_hits_total': 'Counter - ç¼“å­˜å‘½ä¸­æ€»æ•°',
    'cache_misses_total': 'Counter - ç¼“å­˜æœªå‘½ä¸­æ€»æ•°',
    'cache_hit_ratio': 'Gauge - ç¼“å­˜å‘½ä¸­ç‡',
    'cache_size_bytes': 'Gauge - ç¼“å­˜å¤§å°',
    
    # APIæŒ‡æ ‡
    'api_requests_total': 'Counter - APIè¯·æ±‚æ€»æ•°',
    'api_response_time': 'Histogram - APIå“åº”æ—¶é—´',
    'api_errors_total': 'Counter - APIé”™è¯¯æ€»æ•°',
    
    # ä¸šåŠ¡æŒ‡æ ‡
    'valuation_calculations_total': 'Counter - ä¼°å€¼è®¡ç®—æ€»æ•°',
    'valuation_calculation_time': 'Histogram - ä¼°å€¼è®¡ç®—æ—¶é—´',
    'active_users': 'Gauge - æ´»è·ƒç”¨æˆ·æ•°'
}
```

### 6.2 å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# å‘Šè­¦è§„åˆ™é…ç½®
alert_rules:
  # æ•°æ®æºå‘Šè­¦
  data_source_down:
    metric: data_source_availability
    condition: "< 0.5"
    duration: "2m"
    severity: critical
    message: "æ•°æ®æº {{$labels.source}} ä¸å¯ç”¨"
    
  data_source_slow:
    metric: data_source_response_time
    condition: "> 10"
    duration: "5m"
    severity: warning
    message: "æ•°æ®æº {{$labels.source}} å“åº”ç¼“æ…¢"
    
  high_error_rate:
    metric: rate(data_source_errors_total[5m])
    condition: "> 0.1"
    duration: "3m"
    severity: warning
    message: "æ•°æ®æº {{$labels.source}} é”™è¯¯ç‡è¿‡é«˜"
  
  # ç¼“å­˜å‘Šè­¦
  low_cache_hit_ratio:
    metric: cache_hit_ratio
    condition: "< 0.7"
    duration: "10m"
    severity: warning
    message: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {{$value}}"
    
  cache_size_high:
    metric: cache_size_bytes
    condition: "> 8GB"
    duration: "5m"
    severity: warning
    message: "ç¼“å­˜ä½¿ç”¨é‡è¿‡é«˜: {{$value}}"
  
  # APIå‘Šè­¦
  api_response_slow:
    metric: api_response_time
    condition: "> 5"
    duration: "5m"
    severity: warning
    message: "APIå“åº”æ—¶é—´è¿‡é•¿: {{$value}}s"
    
  api_error_rate_high:
    metric: rate(api_errors_total[5m])
    condition: "> 0.05"
    duration: "3m"
    severity: critical
    message: "APIé”™è¯¯ç‡è¿‡é«˜: {{$value}}"

# å‘Šè­¦é€šçŸ¥é…ç½®
notification_channels:
  email:
    enabled: true
    smtp_server: "smtp.example.com"
    recipients: ["admin@example.com"]
    
  slack:
    enabled: true
    webhook_url: "https://hooks.slack.com/services/..."
    channel: "#alerts"
    
  webhook:
    enabled: false
    url: "https://api.example.com/alerts"
```

### 6.3 ç›‘æ§ä»ªè¡¨æ¿

```python
class MonitoringDashboard:
    """ç›‘æ§ä»ªè¡¨æ¿"""
    
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics_collector = metrics_collector
        self.dashboard_config = self._load_dashboard_config()
    
    def _load_dashboard_config(self) -> Dict[str, Any]:
        """åŠ è½½ä»ªè¡¨æ¿é…ç½®"""
        return {
            'panels': [
                {
                    'title': 'æ•°æ®æºçŠ¶æ€',
                    'type': 'stat',
                    'metrics': ['data_source_availability'],
                    'thresholds': [0.9, 0.95]
                },
                {
                    'title': 'å“åº”æ—¶é—´è¶‹åŠ¿',
                    'type': 'graph',
                    'metrics': ['data_source_response_time', 'api_response_time'],
                    'time_range': '1h'
                },
                {
                    'title': 'ç¼“å­˜æ€§èƒ½',
                    'type': 'graph',
                    'metrics': ['cache_hit_ratio', 'cache_hits_total', 'cache_misses_total'],
                    'time_range': '6h'
                },
                {
                    'title': 'é”™è¯¯ç‡ç»Ÿè®¡',
                    'type': 'graph',
                    'metrics': ['data_source_errors_total', 'api_errors_total'],
                    'time_range': '24h'
                },
                {
                    'title': 'ä¸šåŠ¡æŒ‡æ ‡',
                    'type': 'stat',
                    'metrics': ['valuation_calculations_total', 'active_users'],
                    'time_range': '1d'
                }
            ]
        }
    
    async def get_dashboard_data(self) -> Dict[str, Any]:
        """è·å–ä»ªè¡¨æ¿æ•°æ®"""
        dashboard_data = {
            'timestamp': datetime.now().isoformat(),
            'panels': []
        }
        
        for panel_config in self.dashboard_config['panels']:
            panel_data = await self._get_panel_data(panel_config)
            dashboard_data['panels'].append(panel_data)
        
        return dashboard_data
    
    async def _get_panel_data(self, panel_config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–é¢æ¿æ•°æ®"""
        panel_data = {
            'title': panel_config['title'],
            'type': panel_config['type'],
            'data': []
        }
        
        for metric_name in panel_config['metrics']:
            metric_data = await self._get_metric_data(
                metric_name, 
                panel_config.get('time_range', '1h')
            )
            panel_data['data'].append(metric_data)
        
        return panel_data
    
    async def _get_metric_data(self, metric_name: str, time_range: str) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ•°æ®"""
        # è¿™é‡Œåº”è¯¥ä»æ—¶åºæ•°æ®åº“æŸ¥è¯¢æ•°æ®
        # ä¸ºäº†ç¤ºä¾‹ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        return {
            'metric': metric_name,
            'values': [],  # å®é™…çš„æ—¶åºæ•°æ®ç‚¹
            'current_value': 0.0,
            'trend': 'stable'  # up, down, stable
        }
```

## 7. é…ç½®ç®¡ç†

### 7.1 ç¯å¢ƒé…ç½®æ–‡ä»¶

```bash
# .env é…ç½®æ–‡ä»¶

# ==================== æ•°æ®æºé…ç½® ====================
# ä¸»æ•°æ®æºé€‰æ‹©: tushare, postgresql
PRIMARY_DATA_SOURCE=tushare

# å¤‡ç”¨æ•°æ®æºé€‰æ‹©: postgresql, tushare
FALLBACK_DATA_SOURCE=postgresql

# å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šæ•°æ®æº (å¯é€‰): tushare, postgresql
# FORCE_DATA_SOURCE=tushare

# ==================== Tushareé…ç½® ====================
TUSHARE_TOKEN=your_tushare_token_here
TUSHARE_API_URL=http://api.tushare.pro
TUSHARE_TIMEOUT=30
TUSHARE_MAX_RETRIES=3
TUSHARE_RETRY_DELAY=1

# APIé…é¢ç®¡ç†
TUSHARE_DAILY_LIMIT=10000
TUSHARE_MINUTE_LIMIT=500
TUSHARE_ENABLE_QUOTA_CHECK=true

# ==================== PostgreSQLé…ç½® ====================
POSTGRESQL_HOST=localhost
POSTGRESQL_PORT=5432
POSTGRESQL_DATABASE=stock_valuation
POSTGRESQL_USERNAME=postgres
POSTGRESQL_PASSWORD=your_password_here
POSTGRESQL_POOL_SIZE=10
POSTGRESQL_MAX_OVERFLOW=20
POSTGRESQL_POOL_TIMEOUT=30

# ==================== Redisç¼“å­˜é…ç½® ====================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_MAX_CONNECTIONS=10
REDIS_SOCKET_TIMEOUT=5

# ==================== ç¼“å­˜ç­–ç•¥é…ç½® ====================
# å¯ç”¨ç¼“å­˜å±‚çº§: memory, redis, file
ENABLE_MEMORY_CACHE=true
ENABLE_REDIS_CACHE=true
ENABLE_FILE_CACHE=true

# ç¼“å­˜ç›®å½•
FILE_CACHE_DIR=./cache

# ç¼“å­˜TTL (ç§’)
CACHE_TTL_STOCK_BASIC_MEMORY=3600
CACHE_TTL_STOCK_BASIC_REDIS=86400
CACHE_TTL_DAILY_BASIC_MEMORY=1800
CACHE_TTL_DAILY_BASIC_REDIS=3600
CACHE_TTL_FINANCIAL_DATA_MEMORY=7200
CACHE_TTL_FINANCIAL_DATA_REDIS=43200

# ==================== å¥åº·æ£€æŸ¥é…ç½® ====================
HEALTH_CHECK_INTERVAL=60
HEALTH_CHECK_TIMEOUT=10
HEALTH_CHECK_RETRY_COUNT=3

# å¥åº·çŠ¶æ€é˜ˆå€¼
HEALTH_RESPONSE_TIME_HEALTHY=2.0
HEALTH_RESPONSE_TIME_DEGRADED=5.0

# ==================== é™çº§ç­–ç•¥é…ç½® ====================
# å¯ç”¨è‡ªåŠ¨é™çº§
ENABLE_AUTO_DEGRADATION=true

# é™çº§è§¦å‘æ¡ä»¶
DEGRADATION_ERROR_RATE_THRESHOLD=0.5
DEGRADATION_RESPONSE_TIME_THRESHOLD=10.0
DEGRADATION_MIN_DURATION_MINUTES=5

# æ¢å¤æ¡ä»¶
RECOVERY_SUCCESS_RATE_THRESHOLD=0.9
RECOVERY_TEST_INTERVAL=30

# ==================== ç›‘æ§å‘Šè­¦é…ç½® ====================
# å¯ç”¨ç›‘æ§
ENABLE_MONITORING=true
MONITORING_INTERVAL=30

# Prometheusé…ç½®
PROMETHEUS_ENABLED=false
PROMETHEUS_HOST=localhost
PROMETHEUS_PORT=9090

# å‘Šè­¦é…ç½®
ALERT_EMAIL_ENABLED=true
ALERT_EMAIL_SMTP_SERVER=smtp.example.com
ALERT_EMAIL_RECIPIENTS=admin@example.com

ALERT_SLACK_ENABLED=false
ALERT_SLACK_WEBHOOK_URL=

# ==================== æ—¥å¿—é…ç½® ====================
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_FILE_PATH=./logs/stock_valuation.log
LOG_MAX_SIZE=100MB
LOG_BACKUP_COUNT=5

# ==================== æ€§èƒ½é…ç½® ====================
# è¿æ¥æ± é…ç½®
CONNECTION_POOL_SIZE=10
CONNECTION_POOL_MAX_OVERFLOW=20
CONNECTION_POOL_TIMEOUT=30

# å¼‚æ­¥é…ç½®
ASYNC_WORKER_COUNT=4
ASYNC_QUEUE_SIZE=1000

# é™æµé…ç½®
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=1000
RATE_LIMIT_BURST_SIZE=100
```

### 7.2 é…ç½®åŠ è½½å™¨

```python
import os
from typing import Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path

@dataclass
class DataSourceConfig:
    """æ•°æ®æºé…ç½®"""
    primary_source: str
    fallback_source: str
    force_source: Optional[str] = None

@dataclass
class TushareConfig:
    """Tushareé…ç½®"""
    token: str
    api_url: str = "http://api.tushare.pro"
    timeout: int = 30
    max_retries: int = 3
    retry_delay: int = 1
    daily_limit: int = 10000
    minute_limit: int = 500
    enable_quota_check: bool = True

@dataclass
class PostgreSQLConfig:
    """PostgreSQLé…ç½®"""
    host: str = "localhost"
    port: int = 5432
    database: str = "stock_valuation"
    username: str = "postgres"
    password: str = ""
    pool_size: int = 10
    max_overflow: int = 20
    pool_timeout: int = 30

@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    enable_memory: bool = True
    enable_redis: bool = True
    enable_file: bool = True
    file_cache_dir: str = "./cache"
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    redis_password: str = ""

class ConfigLoader:
    """é…ç½®åŠ è½½å™¨"""
    
    def __init__(self, env_file: str = ".env"):
        self.env_file = env_file
        self._load_env_file()
    
    def _load_env_file(self):
        """åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶"""
        if Path(self.env_file).exists():
            with open(self.env_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
    
    def get_data_source_config(self) -> DataSourceConfig:
        """è·å–æ•°æ®æºé…ç½®"""
        return DataSourceConfig(
            primary_source=os.getenv('PRIMARY_DATA_SOURCE', 'tushare'),
            fallback_source=os.getenv('FALLBACK_DATA_SOURCE', 'postgresql'),
            force_source=os.getenv('FORCE_DATA_SOURCE')
        )
    
    def get_tushare_config(self) -> TushareConfig:
        """è·å–Tushareé…ç½®"""
        token = os.getenv('TUSHARE_TOKEN')
        if not token:
            raise ValueError("TUSHARE_TOKEN is required")
        
        return TushareConfig(
            token=token,
            api_url=os.getenv('TUSHARE_API_URL', 'http://api.tushare.pro'),
            timeout=int(os.getenv('TUSHARE_TIMEOUT', '30')),
            max_retries=int(os.getenv('TUSHARE_MAX_RETRIES', '3')),
            retry_delay=int(os.getenv('TUSHARE_RETRY_DELAY', '1')),
            daily_limit=int(os.getenv('TUSHARE_DAILY_LIMIT', '10000')),
            minute_limit=int(os.getenv('TUSHARE_MINUTE_LIMIT', '500')),
            enable_quota_check=os.getenv('TUSHARE_ENABLE_QUOTA_CHECK', 'true').lower() == 'true'
        )
    
    def get_postgresql_config(self) -> PostgreSQLConfig:
        """è·å–PostgreSQLé…ç½®"""
        return PostgreSQLConfig(
            host=os.getenv('POSTGRESQL_HOST', 'localhost'),
            port=int(os.getenv('POSTGRESQL_PORT', '5432')),
            database=os.getenv('POSTGRESQL_DATABASE', 'stock_valuation'),
            username=os.getenv('POSTGRESQL_USERNAME', 'postgres'),
            password=os.getenv('POSTGRESQL_PASSWORD', ''),
            pool_size=int(os.getenv('POSTGRESQL_POOL_SIZE', '10')),
            max_overflow=int(os.getenv('POSTGRESQL_MAX_OVERFLOW', '20')),
            pool_timeout=int(os.getenv('POSTGRESQL_POOL_TIMEOUT', '30'))
        )
    
    def get_cache_config(self) -> CacheConfig:
        """è·å–ç¼“å­˜é…ç½®"""
        return CacheConfig(
            enable_memory=os.getenv('ENABLE_MEMORY_CACHE', 'true').lower() == 'true',
            enable_redis=os.getenv('ENABLE_REDIS_CACHE', 'true').lower() == 'true',
            enable_file=os.getenv('ENABLE_FILE_CACHE', 'true').lower() == 'true',
            file_cache_dir=os.getenv('FILE_CACHE_DIR', './cache'),
            redis_host=os.getenv('REDIS_HOST', 'localhost'),
            redis_port=int(os.getenv('REDIS_PORT', '6379')),
            redis_db=int(os.getenv('REDIS_DB', '0')),
            redis_password=os.getenv('REDIS_PASSWORD', '')
        )
    
    def get_full_config(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´é…ç½®"""
        return {
            'data_source': self.get_data_source_config(),
            'tushare': self.get_tushare_config(),
            'postgresql': self.get_postgresql_config(),
            'cache': self.get_cache_config(),
            'monitoring': {
                'enabled': os.getenv('ENABLE_MONITORING', 'true').lower() == 'true',
                'interval': int(os.getenv('MONITORING_INTERVAL', '30'))
            },
            'logging': {
                'level': os.getenv('LOG_LEVEL', 'INFO'),
                'format': os.getenv('LOG_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'),
                'file_path': os.getenv('LOG_FILE_PATH', './logs/stock_valuation.log')
            }
        }
```

## 8. éƒ¨ç½²æ–¹æ¡ˆ

### 8.1 Dockerå®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºç¼“å­˜ç›®å½•
RUN mkdir -p /app/cache /app/logs

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  stock-valuation:
    build: .
    ports:
      - "8000:8000"
    environment:
      - PRIMARY_DATA_SOURCE=tushare
      - FALLBACK_DATA_SOURCE=postgresql
      - TUSHARE_TOKEN=${TUSHARE_TOKEN}
      - POSTGRESQL_HOST=postgres
      - REDIS_HOST=redis
    depends_on:
      - postgres
      - redis
    volumes:
      - ./cache:/app/cache
      - ./logs:/app/logs
    restart: unless-stopped
    networks:
      - stock-network

  postgres:
    image: postgres:13
    environment:
      - POSTGRES_DB=stock_valuation
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - stock-network

  redis:
    image: redis:6-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - stock-network

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped
    networks:
      - stock-network

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped
    networks:
      - stock-network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  stock-network:
    driver: bridge
```

### 8.2 Kuberneteséƒ¨ç½²

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stock-valuation
  labels:
    app: stock-valuation
spec:
  replicas: 3
  selector:
    matchLabels:
      app: stock-valuation
  template:
    metadata:
      labels:
        app: stock-valuation
    spec:
      containers:
      - name: stock-valuation
        image: stock-valuation:latest
        ports:
        - containerPort: 8000
        env:
        - name: PRIMARY_DATA_SOURCE
          value: "tushare"
        - name: FALLBACK_DATA_SOURCE
          value: "postgresql"
        - name: TUSHARE_TOKEN
          valueFrom:
            secretKeyRef:
              name: tushare-secret
              key: token
        - name: POSTGRESQL_HOST
          value: "postgres-service"
        - name: REDIS_HOST
          value: "redis-service"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: cache-volume
          mountPath: /app/cache
        - name: logs-volume
          mountPath: /app/logs
      volumes:
      - name: cache-volume
        emptyDir: {}
      - name: logs-volume
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: stock-valuation-service
spec:
  selector:
    app: stock-valuation
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

## 9. å®æ–½è®¡åˆ’

### 9.1 å®æ–½é˜¶æ®µåˆ’åˆ†

#### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¶æ„æ­å»º (1-2å‘¨)

**ç›®æ ‡**: å»ºç«‹æ··åˆæ•°æ®æºçš„åŸºç¡€æ¡†æ¶

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»ºDataSourceManageræ ¸å¿ƒç±»
- [ ] å®ç°TushareDataFetcher
- [ ] å®ç°PostgreSQLDataFetcher
- [ ] å»ºç«‹åŸºç¡€çš„é…ç½®ç®¡ç†ç³»ç»Ÿ
- [ ] å®ç°ç®€å•çš„æ•°æ®æºåˆ‡æ¢é€»è¾‘

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤Ÿé€šè¿‡é…ç½®åˆ‡æ¢æ•°æ®æº
- åŸºæœ¬çš„æ•°æ®è·å–åŠŸèƒ½æ­£å¸¸
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°80%

#### ç¬¬äºŒé˜¶æ®µï¼šç¼“å­˜ç³»ç»Ÿå®ç° (1-2å‘¨)

**ç›®æ ‡**: å®ç°å¤šå±‚çº§ç¼“å­˜æœºåˆ¶

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç°CacheManagerç±»
- [ ] é›†æˆRedisç¼“å­˜
- [ ] å®ç°æœ¬åœ°æ–‡ä»¶ç¼“å­˜
- [ ] å»ºç«‹ç¼“å­˜å¤±æ•ˆç­–ç•¥
- [ ] ä¼˜åŒ–ç¼“å­˜é”®ç”Ÿæˆç®—æ³•

**éªŒæ”¶æ ‡å‡†**:
- ç¼“å­˜å‘½ä¸­ç‡è¾¾åˆ°70%ä»¥ä¸Š
- ç¼“å­˜æ•°æ®ä¸€è‡´æ€§ä¿è¯
- ç¼“å­˜æ€§èƒ½æµ‹è¯•é€šè¿‡

#### ç¬¬ä¸‰é˜¶æ®µï¼šå¥åº·æ£€æŸ¥å’Œé™çº§ (1å‘¨)

**ç›®æ ‡**: å®ç°è‡ªåŠ¨å¥åº·æ£€æŸ¥å’Œé™çº§æœºåˆ¶

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç°HealthCheckerç±»
- [ ] å»ºç«‹æ•°æ®æºçŠ¶æ€ç›‘æ§
- [ ] å®ç°è‡ªåŠ¨é™çº§é€»è¾‘
- [ ] å»ºç«‹æ¢å¤æœºåˆ¶
- [ ] å®ç°é”™è¯¯å¤„ç†ç­–ç•¥

**éªŒæ”¶æ ‡å‡†**:
- æ•°æ®æºæ•…éšœè‡ªåŠ¨æ£€æµ‹
- é™çº§åˆ‡æ¢æ—¶é—´<5ç§’
- æ¢å¤æœºåˆ¶æ­£å¸¸å·¥ä½œ

#### ç¬¬å››é˜¶æ®µï¼šç›‘æ§å‘Šè­¦ç³»ç»Ÿ (1å‘¨)

**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„ç›‘æ§å‘Šè­¦ä½“ç³»

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç°MetricsCollector
- [ ] é›†æˆPrometheusç›‘æ§
- [ ] å»ºç«‹å‘Šè­¦è§„åˆ™
- [ ] åˆ›å»ºç›‘æ§ä»ªè¡¨æ¿
- [ ] å®ç°å‘Šè­¦é€šçŸ¥

**éªŒæ”¶æ ‡å‡†**:
- å…³é”®æŒ‡æ ‡ç›‘æ§è¦†ç›–
- å‘Šè­¦åŠæ—¶å‡†ç¡®
- ä»ªè¡¨æ¿æ•°æ®å®Œæ•´

#### ç¬¬äº”é˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯• (1-2å‘¨)

**ç›®æ ‡**: ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å’Œå…¨é¢æµ‹è¯•

**ä»»åŠ¡æ¸…å•**:
- [ ] æ€§èƒ½å‹åŠ›æµ‹è¯•
- [ ] å¹¶å‘å®‰å…¨æµ‹è¯•
- [ ] æ•…éšœæ¢å¤æµ‹è¯•
- [ ] æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
- [ ] ç”¨æˆ·éªŒæ”¶æµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- ç³»ç»Ÿå“åº”æ—¶é—´<2ç§’
- å¹¶å‘ç”¨æˆ·æ•°>100
- æ•°æ®å‡†ç¡®æ€§99.9%
- ç³»ç»Ÿå¯ç”¨æ€§99.5%

### 9.2 é£é™©æ§åˆ¶æªæ–½

#### æŠ€æœ¯é£é™©

| é£é™©é¡¹ | é£é™©ç­‰çº§ | å½±å“ | åº”å¯¹æªæ–½ |
|--------|----------|------|----------|
| Tushare APIå˜æ›´ | ä¸­ | æ•°æ®è·å–å¤±è´¥ | ç‰ˆæœ¬é”å®šã€é€‚é…å±‚è®¾è®¡ |
| æ•°æ®æºåŒæ­¥å»¶è¿Ÿ | ä¸­ | æ•°æ®ä¸ä¸€è‡´ | æ—¶é—´æˆ³æ ¡éªŒã€å¢é‡åŒæ­¥ |
| ç¼“å­˜æ•°æ®è¿‡æœŸ | ä½ | æ€§èƒ½ä¸‹é™ | æ™ºèƒ½é¢„åŠ è½½ã€è¿‡æœŸç­–ç•¥ |
| å¹¶å‘è®¿é—®å†²çª | ä¸­ | æ•°æ®é”™è¯¯ | é”æœºåˆ¶ã€é˜Ÿåˆ—ç®¡ç† |

#### ä¸šåŠ¡é£é™©

| é£é™©é¡¹ | é£é™©ç­‰çº§ | å½±å“ | åº”å¯¹æªæ–½ |
|--------|----------|------|----------|
| æ•°æ®æºæˆæœ¬ä¸Šå‡ | ä¸­ | è¿è¥æˆæœ¬å¢åŠ  | æˆæœ¬ç›‘æ§ã€ç”¨é‡ä¼˜åŒ– |
| æœåŠ¡ä¸­æ–­ | é«˜ | ä¸šåŠ¡åœæ­¢ | å¤šé‡å¤‡ä»½ã€å¿«é€Ÿæ¢å¤ |
| æ•°æ®è´¨é‡é—®é¢˜ | ä¸­ | è®¡ç®—é”™è¯¯ | æ•°æ®æ ¡éªŒã€è´¨é‡ç›‘æ§ |

### 9.3 æˆåŠŸæŒ‡æ ‡

#### æŠ€æœ¯æŒ‡æ ‡
- **ç³»ç»Ÿå¯ç”¨æ€§**: â‰¥99.5%
- **å“åº”æ—¶é—´**: â‰¤2ç§’ (95%è¯·æ±‚)
- **ç¼“å­˜å‘½ä¸­ç‡**: â‰¥70%
- **é”™è¯¯ç‡**: â‰¤0.1%
- **æ•°æ®å‡†ç¡®æ€§**: â‰¥99.9%

#### ä¸šåŠ¡æŒ‡æ ‡
- **APIè°ƒç”¨æˆæœ¬**: é™ä½60%
- **æ•°æ®æ›´æ–°é¢‘ç‡**: æå‡50%
- **ç”¨æˆ·æ»¡æ„åº¦**: â‰¥90%
- **ç³»ç»Ÿç»´æŠ¤æˆæœ¬**: é™ä½40%

## 10. æ€»ç»“

### 10.1 æ¶æ„ä¼˜åŠ¿æ€»ç»“

1. **é«˜å¯é æ€§**: åŒæ•°æ®æºä¿éšœï¼Œå•ç‚¹æ•…éšœé£é™©é™ä½90%
2. **é«˜æ€§èƒ½**: å¤šå±‚ç¼“å­˜æœºåˆ¶ï¼Œæ•°æ®è®¿é—®é€Ÿåº¦æå‡70%
3. **æˆæœ¬ä¼˜åŒ–**: æ™ºèƒ½APIè°ƒç”¨ç®¡ç†ï¼Œæˆæœ¬é™ä½60%
4. **æ˜“ç»´æŠ¤**: æ¨¡å—åŒ–è®¾è®¡ï¼Œç»´æŠ¤æˆæœ¬é™ä½40%
5. **å¯æ‰©å±•**: æ”¯æŒæ–°æ•°æ®æºæ¥å…¥ï¼Œæ‰©å±•æ€§å¼º

### 10.2 å…³é”®æŠ€æœ¯åˆ›æ–°

- **æ™ºèƒ½æ•°æ®æºè·¯ç”±**: åŸºäºå¥åº·çŠ¶æ€å’Œæ€§èƒ½çš„åŠ¨æ€é€‰æ‹©
- **å¤šå±‚çº§ç¼“å­˜**: å†…å­˜+Redis+æ–‡ä»¶çš„ä¸‰çº§ç¼“å­˜ä½“ç³»
- **è‡ªåŠ¨é™çº§æ¢å¤**: æ— äººå·¥å¹²é¢„çš„æ•…éšœå¤„ç†æœºåˆ¶
- **å®æ—¶ç›‘æ§å‘Šè­¦**: å…¨æ–¹ä½çš„ç³»ç»ŸçŠ¶æ€ç›‘æ§

### 10.3 åç»­å‘å±•æ–¹å‘

1. **AIé©±åŠ¨ä¼˜åŒ–**: åŸºäºæœºå™¨å­¦ä¹ çš„ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
2. **å¤šäº‘éƒ¨ç½²**: æ”¯æŒå¤šäº‘ç¯å¢ƒçš„æ•°æ®æºåˆ†å¸ƒ
3. **å®æ—¶æµå¤„ç†**: é›†æˆæµå¼æ•°æ®å¤„ç†èƒ½åŠ›
4. **è¾¹ç¼˜è®¡ç®—**: æ”¯æŒè¾¹ç¼˜èŠ‚ç‚¹çš„æ•°æ®ç¼“å­˜

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ  
**ä¸‹æ¬¡å®¡æ ¸**: 2025å¹´3æœˆ
